===> User
uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
===> Configuring ...
===> Running preflight checks ... 
===> Check if Kafka is healthy ...
Using log4j config /etc/cp-base-new/log4j.properties
===> Launching ... 
===> Launching kafka-connect ... 
[2025-02-05 00:03:01,908] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli)
....
[2025-02-05 00:05:47,358] INFO 172.19.0.1 - - [05/Feb/2025:00:05:47 +0000] "PUT /connectors/pg-connector/config HTTP/1.1" 500 110 "-" "curl/7.84.0" 129 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:05:49,334] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:05:49 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/7.61.1" 8 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:05:54,434] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:05:54 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/7.61.1" 9 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:05:59,497] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:05:59 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/7.61.1" 5 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:04,596] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:04 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/7.61.1" 5 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:09,658] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:09 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/7.61.1" 5 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:14,835] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:14 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/7.61.1" 41 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:18,240] INFO Loading the custom source info struct maker plugin: io.debezium.connector.postgresql.PostgresSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig)
[2025-02-05 00:06:18,376] INFO Successfully tested connection for jdbc:postgresql://postgres:5432/customers with user 'postgres-user' (io.debezium.connector.postgresql.PostgresConnector)
[2025-02-05 00:06:18,400] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection)
[2025-02-05 00:06:18,404] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2025-02-05 00:06:18,426] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Connector pg-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,428] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,428] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,441] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Successfully joined group with generation Generation{generationId=2, memberId='connect-localhost:8083-979bce0b-a173-4fe9-a06e-881678048f69', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,450] INFO 172.19.0.1 - - [05/Feb/2025:00:06:18 +0000] "PUT /connectors/pg-connector/config HTTP/1.1" 201 741 "-" "curl/7.84.0" 337 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:18,463] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Successfully synced group in generation Generation{generationId=2, memberId='connect-localhost:8083-979bce0b-a173-4fe9-a06e-881678048f69', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,464] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-localhost:8083-979bce0b-a173-4fe9-a06e-881678048f69', leaderUrl='http://localhost:8083/', offset=2, connectorIds=[pg-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,465] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,467] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Starting connector pg-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,470] INFO Creating connector pg-connector of type io.debezium.connector.postgresql.PostgresConnector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,471] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2025-02-05 00:06:18,471] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,473] INFO EnrichedSourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig$EnrichedSourceConnectorConfig)
[2025-02-05 00:06:18,473] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,478] INFO Instantiated connector pg-connector with version 3.0.6.Final of type class io.debezium.connector.postgresql.PostgresConnector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,478] INFO Finished creating connector pg-connector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,479] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,491] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2025-02-05 00:06:18,492] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,492] INFO EnrichedSourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig$EnrichedSourceConnectorConfig)
[2025-02-05 00:06:18,493] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,515] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Tasks [pg-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,517] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,517] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,519] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Successfully joined group with generation Generation{generationId=3, memberId='connect-localhost:8083-979bce0b-a173-4fe9-a06e-881678048f69', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,523] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Successfully synced group in generation Generation{generationId=3, memberId='connect-localhost:8083-979bce0b-a173-4fe9-a06e-881678048f69', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[2025-02-05 00:06:18,524] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-localhost:8083-979bce0b-a173-4fe9-a06e-881678048f69', leaderUrl='http://localhost:8083/', offset=4, connectorIds=[pg-connector], taskIds=[pg-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,525] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,526] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Starting task pg-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,529] INFO Creating task pg-connector-0 (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,530] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = pg-connector
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig)
[2025-02-05 00:06:18,530] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = pg-connector
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,534] INFO TaskConfig values: 
	task.class = class io.debezium.connector.postgresql.PostgresConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig)
[2025-02-05 00:06:18,536] INFO Instantiated task pg-connector-0 with version 3.0.6.Final of type io.debezium.connector.postgresql.PostgresConnectorTask (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,540] INFO AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://schema-registry:8081/]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.connect.avro.AvroConverterConfig)
[2025-02-05 00:06:18,678] INFO KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://schema-registry:8081/]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.kafka.serializers.KafkaAvroSerializerConfig)
[2025-02-05 00:06:18,850] INFO KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://schema-registry:8081/]
	specific.avro.key.type = null
	specific.avro.reader = false
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.kafka.serializers.KafkaAvroDeserializerConfig)
[2025-02-05 00:06:18,894] INFO AvroDataConfig values: 
	allow.optional.map.keys = false
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	flatten.singleton.unions = true
	generalized.sum.type.support = false
	ignore.default.for.nullables = false
	schemas.cache.config = 1000
	scrub.invalid.names = false
 (io.confluent.connect.avro.AvroDataConfig)
[2025-02-05 00:06:18,894] INFO Set up the key converter class io.confluent.connect.avro.AvroConverter for task pg-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,895] INFO AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://schema-registry:8081/]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.connect.avro.AvroConverterConfig)
[2025-02-05 00:06:18,895] INFO KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://schema-registry:8081/]
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.kafka.serializers.KafkaAvroSerializerConfig)
[2025-02-05 00:06:18,896] INFO KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.cache.expiry.buffer.seconds = 300
	bearer.auth.client.id = null
	bearer.auth.client.secret = null
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.custom.provider.class = null
	bearer.auth.identity.pool.id = null
	bearer.auth.issuer.endpoint.url = null
	bearer.auth.logical.cluster = null
	bearer.auth.scope = null
	bearer.auth.scope.claim.name = scope
	bearer.auth.sub.claim.name = sub
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	http.connect.timeout.ms = 60000
	http.read.timeout.ms = 60000
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.cache.size = 1000
	latest.cache.ttl.sec = -1
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	propagate.schema.tags = false
	proxy.host = 
	proxy.port = -1
	rule.actions = []
	rule.executors = []
	rule.service.loader.enable = true
	schema.format = null
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://schema-registry:8081/]
	specific.avro.key.type = null
	specific.avro.reader = false
	specific.avro.value.type = null
	use.latest.version = false
	use.latest.with.metadata = null
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
 (io.confluent.kafka.serializers.KafkaAvroDeserializerConfig)
[2025-02-05 00:06:18,897] INFO AvroDataConfig values: 
	allow.optional.map.keys = false
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	flatten.singleton.unions = true
	generalized.sum.type.support = false
	ignore.default.for.nullables = false
	schemas.cache.config = 1000
	scrub.invalid.names = false
 (io.confluent.connect.avro.AvroDataConfig)
[2025-02-05 00:06:18,897] INFO Set up the value converter class io.confluent.connect.avro.AvroConverter for task pg-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,897] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task pg-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,903] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2025-02-05 00:06:18,903] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,903] INFO EnrichedSourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig$EnrichedSourceConnectorConfig)
[2025-02-05 00:06:18,904] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:18,906] WARN The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState)
[2025-02-05 00:06:18,907] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:18,907] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [kafka-0:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-pg-connector-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2025-02-05 00:06:18,907] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
[2025-02-05 00:06:18,916] INFO These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
[2025-02-05 00:06:18,916] INFO Kafka version: 7.7.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2025-02-05 00:06:18,916] INFO Kafka commitId: 91d86f33092378c89731b4a9cf1ce5db831a2b07 (org.apache.kafka.common.utils.AppInfoParser)
[2025-02-05 00:06:18,916] INFO Kafka startTimeMs: 1738713978916 (org.apache.kafka.common.utils.AppInfoParser)
[2025-02-05 00:06:18,920] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [kafka-0:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connector-adminclient-pg-connector-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2025-02-05 00:06:18,926] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, group.id, status.storage.topic, plugin.path, internal.key.converter.schemas.enable, rest.port, config.storage.replication.factor, internal.key.converter, value.converter.schema.registry.url, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, internal.value.converter.schemas.enable, internal.value.converter, offset.storage.replication.factor, config.providers.file.class, offset.storage.topic, value.converter, key.converter, key.converter.schema.registry.url]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
[2025-02-05 00:06:18,930] INFO Kafka version: 7.7.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[2025-02-05 00:06:18,930] INFO Kafka commitId: 91d86f33092378c89731b4a9cf1ce5db831a2b07 (org.apache.kafka.common.utils.AppInfoParser)
[2025-02-05 00:06:18,930] INFO Kafka startTimeMs: 1738713978930 (org.apache.kafka.common.utils.AppInfoParser)
[2025-02-05 00:06:18,950] INFO [Producer clientId=connector-producer-pg-connector-0] Cluster ID: practicum (org.apache.kafka.clients.Metadata)
[2025-02-05 00:06:18,953] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:18,955] INFO Starting PostgresConnectorTask with configuration:
   connector.class = io.debezium.connector.postgresql.PostgresConnector
   database.user = postgres-user
   database.dbname = customers
   transforms.unwrap.delete.handling.mode = rewrite
   topic.creation.default.partitions = -1
   transforms = unwrap
   database.server.name = customers
   database.port = 5432
   table.whitelist = public.customers
   topic.creation.enable = true
   topic.prefix = customers
   task.class = io.debezium.connector.postgresql.PostgresConnectorTask
   database.hostname = postgres
   database.password = ********
   transforms.unwrap.drop.tombstones = false
   name = pg-connector
   topic.creation.default.replication.factor = -1
   transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState
   skipped.operations = none
 (io.debezium.connector.common.BaseSourceTask)
[2025-02-05 00:06:18,956] INFO Loading the custom source info struct maker plugin: io.debezium.connector.postgresql.PostgresSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig)
[2025-02-05 00:06:18,959] INFO Loading the custom topic naming strategy plugin: io.debezium.schema.SchemaTopicNamingStrategy (io.debezium.config.CommonConnectorConfig)
[2025-02-05 00:06:18,987] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection)
[2025-02-05 00:06:19,024] WARN Type [oid:13529, name:_pg_user_mappings] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,024] WARN Type [oid:13203, name:cardinal_number] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,025] WARN Type [oid:13206, name:character_data] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,025] WARN Type [oid:13208, name:sql_identifier] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,025] WARN Type [oid:13214, name:time_stamp] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,025] WARN Type [oid:13216, name:yes_or_no] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,052] INFO No previous offsets found (io.debezium.connector.common.BaseSourceTask)
[2025-02-05 00:06:19,071] WARN Type [oid:13529, name:_pg_user_mappings] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,071] WARN Type [oid:13203, name:cardinal_number] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,071] WARN Type [oid:13206, name:character_data] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,071] WARN Type [oid:13208, name:sql_identifier] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,071] WARN Type [oid:13214, name:time_stamp] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,071] WARN Type [oid:13216, name:yes_or_no] is already mapped (io.debezium.connector.postgresql.TypeRegistry)
[2025-02-05 00:06:19,095] INFO Connector started for the first time. (io.debezium.connector.common.BaseSourceTask)
[2025-02-05 00:06:19,095] INFO No previous offset found (io.debezium.connector.postgresql.PostgresConnectorTask)
[2025-02-05 00:06:19,100] INFO user 'postgres-user' connected to database 'customers' on PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit with roles:
	role 'pg_read_all_settings' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_database_owner' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_stat_scan_tables' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_checkpoint' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_write_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_use_reserved_connections' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'postgres-user' [superuser: true, replication: true, inherit: true, create role: true, create db: true, can log in: true]
	role 'pg_read_all_data' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_write_all_data' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_monitor' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_read_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_create_subscription' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_execute_server_program' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_read_all_stats' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false]
	role 'pg_signal_backend' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] (io.debezium.connector.postgresql.PostgresConnectorTask)
[2025-02-05 00:06:19,104] INFO Obtained valid replication slot ReplicationSlot [active=false, latestFlushedLsn=null, catalogXmin=null] (io.debezium.connector.postgresql.connection.PostgresConnection)
[2025-02-05 00:06:19,124] INFO Creating replication slot with command CREATE_REPLICATION_SLOT "debezium"  LOGICAL decoderbufs  (io.debezium.connector.postgresql.connection.PostgresReplicationConnection)
[2025-02-05 00:06:19,146] INFO Requested thread factory for component PostgresConnector, id = customers named = SignalProcessor (io.debezium.util.Threads)
[2025-02-05 00:06:19,155] INFO Requested thread factory for component PostgresConnector, id = customers named = change-event-source-coordinator (io.debezium.util.Threads)
[2025-02-05 00:06:19,155] INFO Requested thread factory for component PostgresConnector, id = customers named = blocking-snapshot (io.debezium.util.Threads)
[2025-02-05 00:06:19,158] INFO Creating thread debezium-postgresconnector-customers-change-event-source-coordinator (io.debezium.util.Threads)
[2025-02-05 00:06:19,158] INFO WorkerSourceTask{id=pg-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask)
[2025-02-05 00:06:19,161] INFO Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator)
[2025-02-05 00:06:19,161] INFO Context created (io.debezium.pipeline.ChangeEventSourceCoordinator)
[2025-02-05 00:06:19,165] INFO According to the connector configuration data will be snapshotted (io.debezium.connector.postgresql.PostgresSnapshotChangeEventSource)
[2025-02-05 00:06:19,167] INFO Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,167] INFO Setting isolation level (io.debezium.connector.postgresql.PostgresSnapshotChangeEventSource)
[2025-02-05 00:06:19,167] INFO Opening transaction with statement SET TRANSACTION ISOLATION LEVEL REPEATABLE READ; 
SET TRANSACTION SNAPSHOT '00000006-00000002-1'; (io.debezium.connector.postgresql.PostgresSnapshotChangeEventSource)
[2025-02-05 00:06:19,283] INFO Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,286] INFO Adding table public.users to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,287] INFO Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,287] INFO Snapshot step 3 - Locking captured tables [public.users] (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,293] INFO Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,294] INFO Creating initial offset context (io.debezium.connector.postgresql.PostgresOffsetContext)
[2025-02-05 00:06:19,299] INFO Read xlogStart at 'LSN{0/1A43450}' from transaction '745' (io.debezium.connector.postgresql.PostgresOffsetContext)
[2025-02-05 00:06:19,303] INFO Read xlogStart at 'LSN{0/1A43450}' from transaction '745' (io.debezium.connector.postgresql.PostgresSnapshotChangeEventSource)
[2025-02-05 00:06:19,304] INFO Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,304] INFO Reading structure of schema 'public' of catalog 'customers' (io.debezium.connector.postgresql.PostgresSnapshotChangeEventSource)
[2025-02-05 00:06:19,394] INFO Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,395] INFO Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,395] INFO Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,397] INFO For table 'public.users' using select statement: 'SELECT "id", "name", "private_info" FROM "public"."users"' (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,399] INFO Exporting data from table 'public.users' (1 of 1 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,415] INFO 	 Finished exporting 3 records for table 'public.users' (1 of 1 tables); total duration '00:00:00.016' (io.debezium.relational.RelationalSnapshotChangeEventSource)
[2025-02-05 00:06:19,417] INFO Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource)
[2025-02-05 00:06:19,417] INFO Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource)
[2025-02-05 00:06:19,424] INFO Snapshot ended with SnapshotResult [status=COMPLETED, offset=PostgresOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.postgresql.Source:STRUCT}, sourceInfo=source_info[server='customers'db='customers', lsn=LSN{0/1A43450}, txId=745, timestamp=2025-02-05T00:06:19.168981Z, snapshot=FALSE, schema=public, table=users], lastSnapshotRecord=true, lastCompletelyProcessedLsn=null, lastCommitLsn=null, streamingStoppingLsn=null, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator)
[2025-02-05 00:06:19,427] INFO Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator)
[2025-02-05 00:06:19,443] INFO REPLICA IDENTITY for 'public.users' is 'DEFAULT'; UPDATE and DELETE events will contain previous values only for PK columns (io.debezium.connector.postgresql.PostgresSchema)
[2025-02-05 00:06:19,447] INFO SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor)
[2025-02-05 00:06:19,447] INFO Creating thread debezium-postgresconnector-customers-SignalProcessor (io.debezium.util.Threads)
[2025-02-05 00:06:19,447] INFO Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator)
[2025-02-05 00:06:19,447] INFO Retrieved latest position from stored offset 'LSN{0/1A43450}' (io.debezium.connector.postgresql.PostgresStreamingChangeEventSource)
[2025-02-05 00:06:19,448] INFO Looking for WAL restart position for last commit LSN 'null' and last change LSN 'LSN{0/1A43450}' (io.debezium.connector.postgresql.connection.WalPositionLocator)
[2025-02-05 00:06:19,454] INFO Obtained valid replication slot ReplicationSlot [active=false, latestFlushedLsn=LSN{0/1A43450}, catalogXmin=745] (io.debezium.connector.postgresql.connection.PostgresConnection)
[2025-02-05 00:06:19,455] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection)
[2025-02-05 00:06:19,469] INFO Requested thread factory for component PostgresConnector, id = customers named = keep-alive (io.debezium.util.Threads)
[2025-02-05 00:06:19,469] INFO Creating thread debezium-postgresconnector-customers-keep-alive (io.debezium.util.Threads)
[2025-02-05 00:06:19,483] INFO REPLICA IDENTITY for 'public.users' is 'DEFAULT'; UPDATE and DELETE events will contain previous values only for PK columns (io.debezium.connector.postgresql.PostgresSchema)
[2025-02-05 00:06:19,483] INFO Processing messages (io.debezium.connector.postgresql.PostgresStreamingChangeEventSource)
[2025-02-05 00:06:19,957] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:19 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 4 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:20,480] INFO The task will send records to topic 'customers.public.users' for the first time. Checking whether topic exists (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask)
[2025-02-05 00:06:20,507] INFO Creating topic 'customers.public.users' (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask)
[2025-02-05 00:06:20,544] INFO Created topic (name=customers.public.users, numPartitions=-1, replicationFactor=-1, replicasAssignments=null, configs={}) on brokers at kafka-0:9092 (org.apache.kafka.connect.util.TopicAdmin)
[2025-02-05 00:06:20,544] INFO Created topic '(name=customers.public.users, numPartitions=-1, replicationFactor=-1, replicasAssignments=null, configs={})' using creation group TopicCreationGroup{name='default', inclusionPattern=.*, exclusionPattern=, numPartitions=-1, replicationFactor=-1, otherConfigs={}} (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask)
[2025-02-05 00:06:20,549] WARN [Producer clientId=connector-producer-pg-connector-0] Error while fetching metadata with correlation id 4 : {customers.public.users=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2025-02-05 00:06:20,669] WARN [Producer clientId=connector-producer-pg-connector-0] Error while fetching metadata with correlation id 5 : {customers.public.users=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)
[2025-02-05 00:06:23,233] INFO Loading the custom source info struct maker plugin: io.debezium.connector.postgresql.PostgresSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig)
[2025-02-05 00:06:23,247] INFO Successfully tested connection for jdbc:postgresql://postgres:5432/customers with user 'postgres-user' (io.debezium.connector.postgresql.PostgresConnector)
[2025-02-05 00:06:23,252] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection)
[2025-02-05 00:06:23,253] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig)
[2025-02-05 00:06:23,271] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Connector pg-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:23,271] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Handling connector-only config update by restarting connector pg-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:23,271] INFO Stopping connector pg-connector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:23,271] INFO Scheduled shutdown for WorkerConnector{id=pg-connector} (org.apache.kafka.connect.runtime.WorkerConnector)
[2025-02-05 00:06:23,272] INFO Completed shutdown for WorkerConnector{id=pg-connector} (org.apache.kafka.connect.runtime.WorkerConnector)
[2025-02-05 00:06:23,275] INFO [Worker clientId=connect-localhost:8083, groupId=kafka-connect] Starting connector pg-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[2025-02-05 00:06:23,275] INFO Creating connector pg-connector of type io.debezium.connector.postgresql.PostgresConnector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:23,276] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2025-02-05 00:06:23,276] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:23,277] INFO EnrichedSourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig$EnrichedSourceConnectorConfig)
[2025-02-05 00:06:23,278] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:23,280] INFO Instantiated connector pg-connector with version 3.0.6.Final of type class io.debezium.connector.postgresql.PostgresConnector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:23,280] INFO Finished creating connector pg-connector (org.apache.kafka.connect.runtime.Worker)
[2025-02-05 00:06:23,280] INFO 172.19.0.1 - - [05/Feb/2025:00:06:23 +0000] "PUT /connectors/pg-connector/config HTTP/1.1" 200 778 "-" "curl/7.84.0" 54 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:23,283] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[2025-02-05 00:06:23,284] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:23,285] INFO EnrichedSourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig$EnrichedSourceConnectorConfig)
[2025-02-05 00:06:23,285] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.postgresql.PostgresConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = pg-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	topic.creation.default.exclude = []
	topic.creation.default.include = [.*]
	topic.creation.default.partitions = -1
	topic.creation.default.replication.factor = -1
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[2025-02-05 00:06:25,030] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:25 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:30,134] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:30 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 16 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:35,249] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:35 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 8 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:40,303] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:40 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 3 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:45,512] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:45 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:50,621] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:50 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 27 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:06:55,686] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:06:55 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 3 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:00,752] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:00 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:05,819] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:05 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 4 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:10,903] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:10 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 9 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:16,021] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:16 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 9 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:18,951] INFO WorkerSourceTask{id=pg-connector-0} Committing offsets for 3 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask)
[2025-02-05 00:07:19,002] INFO Found previous partition offset PostgresPartition [sourcePartition={server=customers}]: {lsn=27538512, txId=745, ts_usec=1738713979168981} (io.debezium.connector.common.BaseSourceTask)
[2025-02-05 00:07:21,171] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:21 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 10 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:26,337] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:26 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 12 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:31,410] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:31 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:36,478] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:36 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 4 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:41,585] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:41 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 10 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:46,700] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:46 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 5 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:51,981] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:51 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 31 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:07:57,068] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:07:57 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 9 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:02,157] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:02 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 11 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:07,435] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:07 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:12,558] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:12 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 11 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:17,712] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:17 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 11 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:22,850] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:22 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 8 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:28,142] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:28 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 12 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:33,236] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:33 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 10 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:34,020] INFO [AdminClient clientId=kafka-connect-shared-admin] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-02-05 00:08:38,290] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:38 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 5 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:41,466] INFO 172.19.0.11 - - [05/Feb/2025:00:08:41 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 2 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:41,782] INFO 172.19.0.11 - - [05/Feb/2025:00:08:41 +0000] "GET /connectors/pg-connector HTTP/1.1" 200 778 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 8 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:43,420] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:43 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 10 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:44,277] INFO 172.19.0.11 - - [05/Feb/2025:00:08:44 +0000] "GET /connectors/pg-connector/status HTTP/1.1" 200 166 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 7 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:48,496] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:48 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 19 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:49,098] INFO 172.19.0.11 - - [05/Feb/2025:00:08:49 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 1 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:49,144] INFO 172.19.0.11 - - [05/Feb/2025:00:08:49 +0000] "GET /connectors/pg-connector HTTP/1.1" 200 778 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 4 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:49,183] INFO 172.19.0.11 - - [05/Feb/2025:00:08:49 +0000] "GET /connectors/pg-connector/status HTTP/1.1" 200 166 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 1 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:53,544] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:53 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 2 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:54,475] INFO 172.19.0.11 - - [05/Feb/2025:00:08:54 +0000] "GET /connectors/pg-connector/config HTTP/1.1" 200 681 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 9 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:08:58,633] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:08:58 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 7 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:09:03,762] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:09:03 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 8 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:09:04,671] INFO 172.19.0.11 - - [05/Feb/2025:00:09:04 +0000] "GET /debezium/transforms HTTP/1.1" 404 49 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 12 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:09:04,704] INFO 172.19.0.11 - - [05/Feb/2025:00:09:04 +0000] "GET /connectors/pg-connector/config HTTP/1.1" 200 681 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 5 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:09:06,706] INFO 172.19.0.11 - - [05/Feb/2025:00:09:06 +0000] "GET /connectors/pg-connector/config HTTP/1.1" 200 681 "-" "Apache-HttpClient/4.5.14 (Java/11.0.21)" 9 (org.apache.kafka.connect.runtime.rest.RestServer)
[2025-02-05 00:09:08,849] INFO [0:0:0:0:0:0:0:1] - - [05/Feb/2025:00:09:08 +0000] "GET /connectors HTTP/1.1" 200 16 "-" "curl/7.61.1" 4 (org.apache.kafka.connect.runtime.rest.RestServer)
